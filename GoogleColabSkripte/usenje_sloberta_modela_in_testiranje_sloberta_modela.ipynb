{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5o21C9CSgvj"
      },
      "source": [
        "# coding=utf-8\n",
        "\n",
        "import glob, os\n",
        "import sys, re\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "mapa = sys.argv[1]\n",
        "stNepravilnoOdprtih = 0\n",
        "raw_text = \"\"\n",
        "file_path = \"\"\n",
        "i = 0\n",
        "file_counter = 0\n",
        "file_in_counter = 0\n",
        "najdaljsiTrStavek = 0\n",
        "katerJeNajdaljsi = \"\"\n",
        "skupnaDolzinaStavkov = 0\n",
        "stStavkov = 0\n",
        "fileNameFolders = glob.glob(\"/content/drive/My Drive/ucenje_BERT/gigafida/tei/GF*\")\n",
        "\n",
        "skip_stevec = 0\n",
        "\n",
        "koliko_besed = 0\n",
        "\n",
        "for fileNameFolder in fileNameFolders:\n",
        "  if skip_stevec < 5:\n",
        "    skip_stevec += 1\n",
        "    continue\n",
        "\n",
        "  fileNames = glob.glob(fileNameFolder + \"/*.xml\")\n",
        "  for fileName in fileNames:\n",
        "\n",
        "    if file_in_counter > 15:\n",
        "      break\n",
        "    file_in_counter += 1\n",
        "\n",
        "    tree = ET.parse(fileName)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    text = \"\"\n",
        "\n",
        "    for root_text in root[1][1]:\n",
        "      for child in root_text:\n",
        "        for child_with_text in child:\n",
        "          if child_with_text.get('name') != 'neardup' and child_with_text.get('name') != 'std_tech_n' and child_with_text.get('name') != 'std_ling_n':\n",
        "            #print(child_with_text.text)\n",
        "            text += child_with_text.text\n",
        "        text += '\\n'\n",
        "\n",
        "    if (i % 10 == 0):\n",
        "        print(i)\n",
        "    i += 1\n",
        "\n",
        "    if not ((text.count('\\n') / len(text) > 0.05) or (sum(c.isdigit() for c in text) / len(text) > 0.03) or (text.count(':') / len(text) > 0.01) or (text.count('(') / len(text) > 0.01)):\n",
        "\n",
        "\n",
        "        #text = text.lower()\n",
        "        text = re.sub('[!@#$:;\\'\\\"“”•«»%&ø×¹]', '', text)\n",
        "        text = re.sub('[öó]', 'o', text)\n",
        "        text = re.sub('[áä]', 'a', text)\n",
        "        text = re.sub('[0123456789]', '1', text)\n",
        "        text = re.sub('ë', 'e', text)\n",
        "        text = re.sub('í', 'i', text)\n",
        "        text = re.sub('ü', 'u', text)\n",
        "        text = re.sub('ć', 'c', text)\n",
        "\n",
        "\n",
        "        #odstranim odvecne presledke pred locili\n",
        "        text0 = \"\"\n",
        "        c1 = ''\n",
        "        c2 = ''\n",
        "        for c in text:\n",
        "            c1 = c\n",
        "            if ((c1 == '.' or c1 == ',' or c1 == '?' or c1 == '!') and c2 != ' ') or (c1 != '.' and c1 != ',' and c1 != '?' and c1 != '!'):\n",
        "                text0 += c2\n",
        "            c2 = c\n",
        "\n",
        "\n",
        "        #odstranim veckratne presledke iz teksta\n",
        "        text1 = \"\"\n",
        "        c1 = ''\n",
        "        c2 = ''\n",
        "        for c in text0:\n",
        "            c1 = c\n",
        "            if not (c1 == ' ' and c2 == ' '):\n",
        "                text1 += c\n",
        "            c2 = c\n",
        "\n",
        "\n",
        "        #odstranim stevilke z vec enicami iz teksta -> vsa stevila so enice\n",
        "        text2 = \"\"\n",
        "        c1 = ''\n",
        "        c2 = ''\n",
        "        for c in text1:\n",
        "            c1 = c\n",
        "            if not (c1 == '1' and c2 == '1'):\n",
        "                text2 += c\n",
        "            c2 = c\n",
        "\n",
        "\n",
        "        #dodam presledke za stevilke\n",
        "        text25 = \"\"\n",
        "        c2 = ''\n",
        "        for c in text2:\n",
        "            if (c2 == '1' and (c != ' ' and c != ',' and c != '.')):\n",
        "                text25 += c2\n",
        "                text25 += ' '\n",
        "            else:\n",
        "                text25 += c2\n",
        "            c2 = c\n",
        "\n",
        "        #dodam presledke po piki, ce manjkajo\n",
        "        text26 = \"\"\n",
        "        c2 = ''\n",
        "        for c in text25:\n",
        "            if ((c2 == ',' or c2 == '.') and (c != ' ' and c != '\\n' and c != '1')):\n",
        "                text26 += c2\n",
        "                text26 += ' '\n",
        "            else:\n",
        "                text26 += c2\n",
        "            c2 = c\n",
        "\n",
        "        text2 = text26\n",
        "        #izlocim stavke z *, +, -\n",
        "        text3 = \"\"\n",
        "        vmesniText = \"\"\n",
        "        bilPlusKratMinusZnak = False\n",
        "        for c in text2:\n",
        "            vmesniText += c\n",
        "            if c == '+' or c == '-' or c == '*':\n",
        "                bilPlusKratMinusZnak = True\n",
        "            elif c == '.' or c == '?' or c == '!':\n",
        "                if not bilPlusKratMinusZnak:\n",
        "                    text3 += vmesniText\n",
        "                bilPlusKratMinusZnak = False\n",
        "                vmesniText = \"\"\n",
        "\n",
        "        text4 = \"\"\n",
        "        splittedText = text3.split('\\n');\n",
        "        for stavek in splittedText:\n",
        "          if len(stavek) > 1:\n",
        "            if \".\" in stavek:\n",
        "              text4 += stavek + \"\\n\"\n",
        "\n",
        "        text6 = text4\n",
        "\n",
        "        text7 = \"\"\n",
        "        c1 = ''\n",
        "        c2 = ''\n",
        "        for c in text6:\n",
        "            c1 = c\n",
        "            if ((c2 == ',' or c2 == '.' or c2 == '!' or c2 == '?') and c1 == '1'):\n",
        "                text7 += ' ' + c\n",
        "            else:\n",
        "                text7 += c\n",
        "            c2 = c\n",
        "\n",
        "        #odstranim se odvecne presledke\n",
        "        text5 = \"\"\n",
        "        c1 = ''\n",
        "        c2 = ''\n",
        "        for c in text7:\n",
        "            c1 = c\n",
        "            if not (c1 == ' ' and c2 == ' '):\n",
        "                text5 += c\n",
        "            c2 = c\n",
        "        #text5 = text4\n",
        "\n",
        "        #print(\"text5 je\" + text5)\n",
        "\n",
        "        if len(text5) > 10:\n",
        "            raw_text += text5\n",
        "            stNepravilnoOdprtih = 0\n",
        "\n",
        "  #print(raw_text)\n",
        "  file_path = \"/content/drive/My Drive/magistrska/poskusno_generiranje_texta/poskusno_generiranje_texta\" + str(file_counter) + \".txt\"\n",
        "  file = open(file_path,\"w\")\n",
        "  file.write(raw_text)\n",
        "  file.close()\n",
        "  raw_text = \"\"\n",
        "  if file_counter > 0:\n",
        "    break\n",
        "  file_counter += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#skupnaDolzinaStavkov /= stStavkov\n",
        "\n",
        "print(\"povprecnaDolzinaStavkov je\", skupnaDolzinaStavkov)\n",
        "print(\"najdalji tr stavek je\", najdaljsiTrStavek)\n",
        "print(katerJeNajdaljsi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwJMy6RewacT"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "from __future__ import division, print_function\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "from io import open\n",
        "import re\n",
        "import sys\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "NUM = '<NUM>'\n",
        "\n",
        "EOS_PUNCTS = {\".\": \".PIKA\", \"?\": \".PIKA\", \"!\": \".PIKA\"}\n",
        "INS_PUNCTS = {\",\": \",VEJICA\", \";\": \".PIKA\", \":\": \" \", \"-\": \" \"}\n",
        "\n",
        "forbidden_symbols = re.compile(r\"[\\[\\]\\(\\)\\/\\\\\\>\\<\\=\\+\\_\\*]\")\n",
        "numbers = re.compile(r\"\\d\")\n",
        "multiple_punct = re.compile(r'([\\.\\?\\!\\,\\:\\;\\-])(?:[\\.\\?\\!\\,\\:\\;\\-]){1,}')\n",
        "\n",
        "is_number = lambda x: len(numbers.sub(\"\", x)) / len(x) < 0.6\n",
        "\n",
        "def untokenize(line):\n",
        "    return line.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\"can not\", \"cannot\")\n",
        "\n",
        "def skip(line):\n",
        "\n",
        "    if line.strip() == '':\n",
        "        return True\n",
        "\n",
        "    last_symbol = line[-1]\n",
        "    if not last_symbol in EOS_PUNCTS:\n",
        "        return True\n",
        "\n",
        "    if forbidden_symbols.search(line) is not None:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def process_line(line):\n",
        "\n",
        "    # tokenized line looks like words in array, for example: [\"I\", \"am\", ....]\n",
        "    tokens = word_tokenize(line)\n",
        "    output_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "\n",
        "        if token in INS_PUNCTS:\n",
        "            output_tokens.append(INS_PUNCTS[token])\n",
        "        elif token in EOS_PUNCTS:\n",
        "            output_tokens.append(EOS_PUNCTS[token])\n",
        "        elif is_number(token):\n",
        "            output_tokens.append(NUM)\n",
        "        else:\n",
        "            # we get bach normal sentance with tokens\n",
        "            output_tokens.append(token)\n",
        "\n",
        "    return untokenize(\" \".join(output_tokens) + \" \")\n",
        "\n",
        "skipped = 0\n",
        "allLines = 0\n",
        "\n",
        "import glob, os\n",
        "fileNames = glob.glob(\"/content/drive/My Drive/ucenje_BERT/proba_pred_zagovorom/*\")\n",
        "\n",
        "file_counter = 0\n",
        "for filename in fileNames:\n",
        "  noviFilename = \"/content/drive/My Drive/ucenje_BERT/proba_pred_zagovorom/vmesniPreparedText\" + str(file_counter) + \".txt\"\n",
        "  file_counter += 1\n",
        "  with open(noviFilename, 'w', encoding='utf-8') as out_txt:\n",
        "      with open(filename, 'r', encoding='utf-8') as text:\n",
        "\n",
        "        print(\"vsaj tuki sm!!\")\n",
        "        for line in text:\n",
        "            #print(\"line je --> \", line)\n",
        "            line = line.replace(\"\\\"\", \"\").strip()\n",
        "            line = multiple_punct.sub(r\"\\g<1>\", line)\n",
        "\n",
        "            allLines += 1\n",
        "            if skip(line):\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            line = process_line(line)\n",
        "\n",
        "            out_txt.write(line + '\\n')\n",
        "\n",
        "print(\"Skipped %d lines\" % skipped)\n",
        "print(\"All %d lines\" % allLines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM_T2S2x0qVF"
      },
      "source": [
        "pustiVejice = False\n",
        "\n",
        "import glob, os\n",
        "#fileNames = glob.glob(\"/content/drive/My Drive/ucenje_BERT/nova_ucna_mnozica_bert/vmesniPreparedText*\")\n",
        "fileNames = glob.glob(\"/content/drive/My Drive/ucenje_BERT/testna_mnozica_bert/testna_mnozica_halozan/vejica_stavki.txt\")\n",
        "\n",
        "file_counter = 0\n",
        "for filename in fileNames:\n",
        "\n",
        "  output_file = \"/content/drive/My Drive/ucenje_BERT/testna_mnozica_bert/testna_mnozica_halozan/vmesni2PreparedText\" + str(file_counter) + \".txt\"\n",
        "  file_counter += 1\n",
        "  pustiVejice = True\n",
        "\n",
        "  with open(filename, 'r', encoding='utf-8') as input_text:\n",
        "\n",
        "    lines = input_text.read().split('\\n')\n",
        "    all_lines = \"\"\n",
        "\n",
        "    stevec = 0\n",
        "    vseh = len(lines)\n",
        "\n",
        "    for line in lines:\n",
        "\n",
        "        print('it is')\n",
        "\n",
        "        stevec += 1\n",
        "        print('progress: ', stevec / vseh, '%')\n",
        "\n",
        "        if pustiVejice:\n",
        "            line = line.replace(',VEJICA .PIKA', '.').replace(',VEJICA', ',VEJICA').replace('.PIKA', '.').replace('?QUESTIONMARK', '?').replace('<NUM>', '400')\n",
        "        else:\n",
        "            line = line.replace(' ,VEJICA', '').replace('.PIKA', '.').replace('?VPRASAJ', '?')\n",
        "\n",
        "\n",
        "        line = \"[CLS] \" + line + \"[SEP]\\n\"\n",
        "        all_lines += line\n",
        "\n",
        "f = open(output_file, \"w\")\n",
        "f.write(\"%s\" % all_lines)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLgxvrhG2kIB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c991cfc2-1108-452a-dc41-b86fe699b6df"
      },
      "source": [
        "import csv\n",
        "\n",
        "def copy_line_add_mask(line_to_copy, index_add_mask, labels):\n",
        "    copied_line = \"\"\n",
        "    counter = 0\n",
        "    masked_comma = False\n",
        "    mask_next_one = False\n",
        "    added_token_previousely = False\n",
        "    beginning = True\n",
        "    ending = False\n",
        "    added_label_previously = False\n",
        "\n",
        "\n",
        "    for token in line_to_copy:\n",
        "        if index_add_mask != -1:\n",
        "            if counter != 0 and token != ',VEJICA':\n",
        "                copied_line += \" \"\n",
        "\n",
        "            if token != ',VEJICA':\n",
        "                copied_line += token\n",
        "            elif counter == index_add_mask:\n",
        "                masked_comma = True\n",
        "\n",
        "            if counter == index_add_mask:\n",
        "                copied_line += ' [MASK]'\n",
        "            counter += 1\n",
        "        else:\n",
        "            if token == '.':\n",
        "                ending = True\n",
        "\n",
        "            if not ending and not mask_next_one and added_token_previousely:\n",
        "                copied_line += ' [MASK] '\n",
        "            elif ending:\n",
        "                copied_line += ' '\n",
        "\n",
        "            if added_label_previously and not ending and counter > 2 and token != ',VEJICA':\n",
        "                # copied_line += \" \"\n",
        "                labels += \" \"\n",
        "                added_label_previously = False\n",
        "\n",
        "            if token != ',VEJICA':\n",
        "                copied_line += token\n",
        "                if counter != 0:\n",
        "                    added_token_previousely = True\n",
        "                else:\n",
        "                    copied_line += ' '\n",
        "                if not beginning and not ending:\n",
        "                    if not mask_next_one:\n",
        "                        labels += \"0\"\n",
        "                        added_label_previously = True\n",
        "                    else:\n",
        "                        mask_next_one = False\n",
        "                        labels += \"1\"\n",
        "                        added_label_previously = True\n",
        "                elif counter != 0:\n",
        "                    beginning = False\n",
        "            else:\n",
        "                mask_next_one = True\n",
        "                added_token_previousely = True\n",
        "            counter += 1\n",
        "\n",
        "    if index_add_mask != -1:\n",
        "        if masked_comma:\n",
        "            labels += \"1\\n\"#\"VEJICA\\n\"\n",
        "        else:\n",
        "            labels += \"0\\n\"#\"PRESLEDEK\\n\n",
        "    else:\n",
        "        labels += \"\\n\"\n",
        "\n",
        "    return (copied_line, labels)\n",
        "\n",
        "\n",
        "import glob, os\n",
        "fileNames = glob.glob(\"/content/drive/My Drive/ucenje_BERT/testna_mnozica_bert/testna_mnozica_halozan/vmesni2PreparedText*\")\n",
        "\n",
        "file_counter = 0\n",
        "#print(fileNames)\n",
        "\n",
        "for filename in fileNames:\n",
        "\n",
        "  mask_all_in_one_sentence = True\n",
        "  add_training_labels = False\n",
        "\n",
        "  output_file = \"/content/drive/My Drive/ucenje_BERT/testna_mnozica_bert/testna_mnozica_halozan/maskedPreparedText\" + str(file_counter) + \".txt\"\n",
        "  file_counter += 1\n",
        "\n",
        "  add_training_labels = True\n",
        "\n",
        "  skipped_lines = 0\n",
        "  matrix_of_masked_lines = \"\"\n",
        "  matrix_of_labels = \"\"\n",
        "\n",
        "  pripravljen_text_bert_stavki = open(filename, 'r', encoding='utf-8').read()\n",
        "\n",
        "  lines = pripravljen_text_bert_stavki.split('\\n')\n",
        "  stevec = 0\n",
        "  vseh = len(lines)\n",
        "  print(len(lines))\n",
        "\n",
        "  for line in lines:\n",
        "      #print(stevec)\n",
        "      stevec += 1\n",
        "      if stevec % 10000 == 0:\n",
        "        print('progress: ', stevec / vseh, '%')\n",
        "\n",
        "      #if (stevec > 10):\n",
        "      #    break\n",
        "\n",
        "      tokenized_line = line.split(' ')\n",
        "\n",
        "\n",
        "      if tokenized_line[0] != '[CLS]' or tokenized_line[len(tokenized_line) - 1] != '[SEP]':\n",
        "          skipped_lines += 1\n",
        "          continue\n",
        "\n",
        "      if not mask_all_in_one_sentence:\n",
        "          for index in range(1, len(tokenized_line) - 3):\n",
        "              new_masked_line, matrix_of_labels = copy_line_add_mask(tokenized_line, index, matrix_of_labels)\n",
        "              new_masked_line = new_masked_line + \"\\n\"\n",
        "              matrix_of_masked_lines += new_masked_line\n",
        "      else:\n",
        "          new_masked_line, matrix_of_labels = copy_line_add_mask(tokenized_line, -1, matrix_of_labels)\n",
        "          new_masked_line = new_masked_line + \"\\n\"\n",
        "          matrix_of_masked_lines += new_masked_line\n",
        "      if not add_training_labels:\n",
        "          matrix_of_masked_lines += \"\\nT\\n\"\n",
        "\n",
        "\n",
        "  if add_training_labels:\n",
        "      f = open(output_file, \"w\")\n",
        "      print(\"v add new labels\")\n",
        "      #writer = csv.writer(f)\n",
        "\n",
        "      sentance_rows = matrix_of_masked_lines.split('\\n')\n",
        "      label_rows = matrix_of_labels.split('\\n')\n",
        "\n",
        "      assert len(sentance_rows) == len(label_rows)\n",
        "\n",
        "      for i in range (0, len(sentance_rows)):\n",
        "          #print(sentance_rows[i] + \"\\t\" + label_rows[i])\n",
        "          if len(sentance_rows[i].split(' ')) > 1 and len(sentance_rows[i].split(' ')) <= 64:\n",
        "              #print(sentance_rows[i] + \"\\t\" + label_rows[i])\n",
        "              f.write(sentance_rows[i] + \"\\t\" + label_rows[i] + \"\\n\")\n",
        "          #writer.writerow(sentance_rows[i] + \"\\t\" + label_rows[i])\n",
        "      f.close()\n",
        "  else:\n",
        "      f = open(output_file, \"w\")\n",
        "      f.write(\"%s\" % matrix_of_masked_lines)\n",
        "      f.close()\n",
        "      print(\"Lines skipped\", skipped_lines)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "104186\n",
            "progress:  0.0959821857063329 %\n",
            "progress:  0.1919643714126658 %\n",
            "progress:  0.2879465571189987 %\n",
            "progress:  0.3839287428253316 %\n",
            "progress:  0.4799109285316645 %\n",
            "progress:  0.5758931142379974 %\n",
            "progress:  0.6718752999443304 %\n",
            "progress:  0.7678574856506633 %\n",
            "progress:  0.8638396713569961 %\n",
            "progress:  0.959821857063329 %\n",
            "v add new labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4y2rERj967H"
      },
      "source": [
        "!pip install Cython\n",
        "!pip install torch\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRq1GSP8-w4v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51b12056-da05-4458-9a9f-d08983942a31"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA-JXp6J9ody",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07e78343-f1fc-48db-adfc-bb0369de5643"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbB-VCsYEKoK"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = []\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "li = pd.read_csv('/content/drive/My Drive/ucenje_BERT/nova_ucna_mnozica_bert/maskedPreparedText0.txt', delimiter='\\t', header=None, names=['sentence', 'label'])\n",
        "df.append(li)\n",
        "li = pd.read_csv('/content/drive/My Drive/ucenje_BERT/nova_ucna_mnozica_bert/maskedPreparedText1.txt', delimiter='\\t', header=None, names=['sentence', 'label'])\n",
        "df.append(li)\n",
        "li = pd.read_csv('/content/drive/My Drive/ucenje_BERT/nova_ucna_mnozica_bert/maskedPreparedText2.txt', delimiter='\\t', header=None, names=['sentence', 'label'])\n",
        "df.append(li)\n",
        "li = pd.read_csv('/content/drive/My Drive/ucenje_BERT/nova_ucna_mnozica_bert/maskedPreparedText3.txt', delimiter='\\t', header=None, names=['sentence', 'label'])\n",
        "df.append(li)\n",
        "li = pd.read_csv('/content/drive/My Drive/ucenje_BERT/nova_ucna_mnozica_bert/maskedPreparedText4.txt', delimiter='\\t', header=None, names=['sentence', 'label'])\n",
        "df.append(li)\n",
        "\n",
        "df = pd.concat(df, axis=0, ignore_index=True)\n",
        "#\n",
        "df.drop(df.tail(2).index,inplace=True)\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txz764CmEtln"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew4uUqEQFMIX"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "#SLO\n",
        "tokenizer = BertTokenizer.from_pretrained('drive/My Drive/ucenje_BERT/pretrained_slo_bert/', do_lower_case=True)\n",
        "\n",
        "#MULTILINGUAL\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl0rA2C8FYsD"
      },
      "source": [
        "sent = \"[CLS] Ko [MASK] se [MASK] je [MASK] obrnila [MASK] je [MASK] bila [MASK] pretresena . [SEP]\"\n",
        "\n",
        "# Print the original sentence.\n",
        "print(' Original: ', sent)\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sent))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1xL9b8nJHiV"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "stevec = 0\n",
        "\n",
        "for sent in sentences:\n",
        "  stevec +=1\n",
        "  #if stevec > 1000:\n",
        "    #break\n",
        "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "\n",
        "  try:\n",
        "    input_ids110 = tokenizer.encode(sent, add_special_tokens=False)\n",
        "  except:\n",
        "    print(stevec)\n",
        "    print(sent)\n",
        "    print(input_ids110)\n",
        "\n",
        "  # Update the maximum sentence length.\n",
        "  max_len = max(max_len, len(input_ids110))\n",
        "\n",
        "print(stevec)\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkV_zVAwOxav"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "label_list = []\n",
        "stevec = 0\n",
        "stevec_izpuscenih = 0\n",
        "print(len(sentences))\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "  #if stevec > 100:\n",
        "  #  break\n",
        "  stevec += 1\n",
        "\n",
        "  if (stevec % 10000 == 0):\n",
        "    print(\"progress: \", stevec / len(sentences))\n",
        "\n",
        "  #sent = \"[CLS] Ko [MASK] se [MASK] je [MASK] obrnila [MASK] je [MASK] bila [MASK] pretresena . [SEP]\"\n",
        "  # `encode_plus` will:\n",
        "  #   (1) Tokenize the sentence.\n",
        "  #   (2) Prepend the `[CLS]` token to the start.\n",
        "  #   (3) Append the `[SEP]` token to the end.\n",
        "  #   (4) Map tokens to their IDs.\n",
        "  #   (5) Pad or truncate the sentence to `max_length`\n",
        "  #   (6) Create attention masks for [PAD] tokens.\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "                      sent,                       # Sentence to encode.\n",
        "                      add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
        "                      max_length = 128,           # Pad & truncate all sentences.\n",
        "                      pad_to_max_length = True,\n",
        "                      truncation = True,\n",
        "                      return_attention_mask = True,   # Construct attn. masks.\n",
        "                      return_tensors = 'pt',      # Return pytorch tensors.\n",
        "                  )\n",
        "\n",
        "\n",
        "\n",
        "  try:\n",
        "\n",
        "    labels_in = []\n",
        "    cur_labels = labels[stevec - 1].split(' ')\n",
        "\n",
        "    maskedindexesIndexPointer = 0\n",
        "\n",
        "    for token in encoded_dict['input_ids'][0].numpy():\n",
        "      if token == 105:\n",
        "        labels_in.append(int((cur_labels[maskedindexesIndexPointer])))\n",
        "        maskedindexesIndexPointer += 1\n",
        "      else:\n",
        "        labels_in.append(-100)\n",
        "\n",
        "    #print(encoded_dict['input_ids'][0].numpy())\n",
        "\n",
        "    #print(encoded_dict['attention_mask'][0].numpy())\n",
        "\n",
        "    #print(labels_in)\n",
        "\n",
        "    assert len(encoded_dict['input_ids'][0].numpy()) == len(labels_in)\n",
        "\n",
        "    #print(labels_in)\n",
        "\n",
        "    if maskedindexesIndexPointer != len(cur_labels):\n",
        "      #print(sent)\n",
        "      #print(maskedindexesIndexPointer)\n",
        "      #print(encoded_dict['input_ids'])\n",
        "      #print(cur_labels)\n",
        "      stevec_izpuscenih += 0\n",
        "\n",
        "    #print(maskedindexesIndexPointer)\n",
        "    #print(len(cur_labels))\n",
        "\n",
        "    assert maskedindexesIndexPointer == len(cur_labels)\n",
        "    #print(encoded_dict['input_ids'])\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    label_list.append(torch.tensor([labels_in]))\n",
        "\n",
        "  except:\n",
        "    #if not type(labels[stevec - 1]) is float:\n",
        "      #print(labels[stevec - 1].split(' '))\n",
        "    #print(labels[stevec - 1])\n",
        "    #print(encoded_dict['input_ids'])\n",
        "    #print(sent)\n",
        "    stevec_izpuscenih += 1\n",
        "\n",
        "  #break\n",
        "\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "#labels = torch.tensor(labels)\n",
        "labels_torch = torch.cat(label_list, dim = 0)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', tokenizer.tokenize(sentences[0]))\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Labels:', labels_torch[0])\n",
        "print('Original: ', tokenizer.tokenize(sentences[4]))\n",
        "print('Token IDs:', input_ids[4])\n",
        "print('Labels:', labels_torch[4])\n",
        "print('Original: ', tokenizer.tokenize(sentences[400]))\n",
        "print('Token IDs:', input_ids[400])\n",
        "print('Labels:', labels_torch[400])\n",
        "\n",
        "print(\"st izpuscenih\")\n",
        "print(stevec_izpuscenih)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxzN6KGM7NX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e0542989-fb0d-49df-a947-4cf54f4480e7"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels_torch)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.900 * len(dataset))\n",
        "val_and_test_size = len(dataset) - train_size\n",
        "\n",
        "val_size = int(0.5 * val_and_test_size)\n",
        "test_size = int(0.5 * val_and_test_size)\n",
        "\n",
        "print(train_size)\n",
        "print(val_size)\n",
        "print(test_size)\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2389136\n",
            "49774\n",
            "49774\n",
            "2,389,136 training samples\n",
            "49,774 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1lmaK2FXQO3"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset, # The test samples.\n",
        "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whay3pvmXg2p"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertForMaskedLM, AdamW, BertConfig\n",
        "\n",
        "#slo\n",
        "model = BertForMaskedLM.from_pretrained(\n",
        "    'drive/My Drive/ucenje_BERT/pretrained_slo_bert/'\n",
        ")\n",
        "\n",
        "#ne slo\n",
        "#model = BertForMaskedLM.from_pretrained(\n",
        "#    'bert-base-multilingual-cased'\n",
        "#)\n",
        "\n",
        "\"\"\"\n",
        "model = BertForMaskedLM.from_pretrained(\n",
        "    'drive/My Drive/ucenje_BERT/pretrained_slo_bert/',\n",
        "    num_labels = 3,\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJPohjaKE6jM"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertForMaskedLM, AdamW, BertConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpCX1VRiYQsV"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XxY16cuYX6m"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertForMaskedLM, AdamW, BertConfig\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KligVqiZYdeo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dc2d02d0-071f-4f44-9993-57c411390cff"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4.\n",
        "epochs = 4\n",
        "#epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "print(scheduler)\n",
        "print(scheduler.get_last_lr())\n",
        "#print(scheduler.step())\n",
        "#print(scheduler.base_lrs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torch.optim.lr_scheduler.LambdaLR object at 0x7fc282643b00>\n",
            "[2e-05]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqi-2eidYh4l"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US-6KqnrYkPw"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfgWbEZ8Ysp3"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os, time\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "output_dir = 'drive/My Drive/ucenje_BERT/bert_multilingual/'\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "total_stevec = 0\n",
        "stevci = []\n",
        "learningRates = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "\n",
        "        #print(1)\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        loss, logits = model(b_input_ids,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "        total_stevec += 1\n",
        "\n",
        "        if total_stevec % 1000 == 0:\n",
        "          learningRates.append(scheduler.get_last_lr())\n",
        "          stevci.append(total_stevec)\n",
        "\n",
        "          print(scheduler.get_last_lr())\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    stevec = 0\n",
        "\n",
        "    num_right_predictions = 0\n",
        "    num_all_predictions = 0\n",
        "    num_right_comma_predictions = 0\n",
        "    num_all_commas = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        batch_numpy = tuple(t.numpy() for t in batch)\n",
        "        b_input_numpy_ids, b_input_numpy_mask, b_numpy_labels = batch_numpy\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            logits = model(b_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=b_input_mask)\n",
        "\n",
        "            logits2 = logits[0]\n",
        "            for batch_index in range(0, len(b_input_numpy_ids)):\n",
        "            #print(batch_index)\n",
        "              for i in range(0, len(b_input_numpy_ids[batch_index])):\n",
        "                #print(torch.argmax(logits2[0, i]).item())\n",
        "                if b_input_numpy_ids[batch_index][i] == 105:\n",
        "                  if b_numpy_labels[batch_index][i] == torch.argmax(logits2[batch_index, i]).item():\n",
        "                    num_right_predictions += 1\n",
        "                  num_all_predictions += 1\n",
        "\n",
        "                  if b_numpy_labels[batch_index][i] == 1:\n",
        "                    if torch.argmax(logits2[batch_index, i]).item() == 1:\n",
        "                      num_right_comma_predictions += 1\n",
        "                    num_all_commas += 1\n",
        "\n",
        "            # Accumulate the validation loss.\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits[0].detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            pred = np.argmax(logits, axis=2).flatten()\n",
        "            origin = label_ids.flatten()\n",
        "\n",
        "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print('    DONE.')\n",
        "    print('Accuracy: ' + str((num_right_predictions / num_all_predictions) * 100) + '%')\n",
        "    print('Accuracy, comma: ' + str((num_right_comma_predictions / num_all_commas) * 100) + '%')\n",
        "    print(num_right_predictions)\n",
        "    print(num_all_predictions)\n",
        "    print(num_right_comma_predictions)\n",
        "    print(num_all_commas)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(stevci)\n",
        "print(learningRates)\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQc0zeE9nLLw"
      },
      "source": [
        "import os\n",
        "\n",
        "output_dir = 'drive/My Drive/ucenje_BERT/bert_multilingual_cased/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Oag60Exxc87"
      },
      "source": [
        "import re\n",
        "\n",
        "def tokens_to_real_stavek(tokens, kje):\n",
        "  stavek = \"\"\n",
        "  stevec = 1\n",
        "  for token in tokens:\n",
        "    if token != '[MASK]' and token != '[SEP]' and token != '[CLS]':\n",
        "      if stevec == kje:\n",
        "        stavek += '!!!'\n",
        "      if '##' not in token:\n",
        "        stavek += \" \"\n",
        "      stavek += re.sub('#', '', token)\n",
        "      stevec += 1\n",
        "  return stavek"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxsveC-ynNsi"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "stevec = 0\n",
        "\n",
        "num_right_predictions = 0\n",
        "num_all_predictions = 0\n",
        "num_right_comma_predictions = 0\n",
        "num_all_commas = 0\n",
        "should_be_comma = 0\n",
        "should_be_space = 0\n",
        "\n",
        "# Predict\n",
        "\n",
        "print(len(test_dataloader))\n",
        "for batch in test_dataloader:\n",
        "  stevec += 1\n",
        "  #print(stevec)\n",
        "  if stevec < 500:\n",
        "    continue\n",
        "  if stevec % 10 == 0:\n",
        "    print(stevec)\n",
        "  # Add batch to GPU\n",
        "  batch_numpy = tuple(t.numpy() for t in batch)\n",
        "  b_input_numpy_ids, b_input_numpy_mask, b_numpy_labels = batch_numpy\n",
        "\n",
        "\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "\n",
        "\n",
        "  #print(outputs)\n",
        "  logits = outputs[0]\n",
        "  #print(logits[0])\n",
        "  output_labels = []\n",
        "  #batch_index = 9\n",
        "  for batch_index in range(0, len(b_input_numpy_ids)):\n",
        "    #print(batch_index)\n",
        "    for i in range(0, len(b_input_numpy_ids[batch_index])):\n",
        "      #print(torch.argmax(logits[0, i]).item())\n",
        "      if b_input_numpy_ids[batch_index][i] == 103:\n",
        "        if b_numpy_labels[batch_index][i] == torch.argmax(logits[batch_index, i]).item():\n",
        "          num_right_predictions += 1\n",
        "        elif b_numpy_labels[batch_index][i] == 0:\n",
        "          should_be_space += 1\n",
        "        else:\n",
        "          should_be_comma += 1\n",
        "        num_all_predictions += 1\n",
        "\n",
        "        if b_numpy_labels[batch_index][i] == 1:\n",
        "          if torch.argmax(logits[batch_index, i]).item() == 1:\n",
        "            num_right_comma_predictions += 1\n",
        "          num_all_commas += 1\n",
        "\n",
        "  #break\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  #print(np.argmax(logits, axis=1))\n",
        "  #print(label_ids)\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(np.argmax(logits, axis=1).flatten())\n",
        "  true_labels.append(label_ids)\n",
        "  #break\n",
        "  if stevec > 1000:\n",
        "    break\n",
        "\n",
        "print('pravilne: ', num_right_predictions)\n",
        "print('vse: ', num_all_predictions)\n",
        "print('pravilne comma: ', num_right_comma_predictions)\n",
        "print('vse comma: ', num_all_commas)\n",
        "print('should be comma but model didnt place it: ', should_be_comma)\n",
        "print('model predicted comma wrongly: ', should_be_space)\n",
        "\n",
        "print('    DONE.')\n",
        "print('Accuracy: ' + str((num_right_predictions / num_all_predictions) * 100) + '%')\n",
        "print('Accuracy, comma: ' + str((num_right_comma_predictions / num_all_commas) * 100) + '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov0vmgUAno5h"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O5iMRssUQRm"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertForSequenceClassification, BertForMaskedLM, AdamW, BertConfig, BertTokenizer\n",
        "\n",
        "output_dir = 'drive/My Drive/ucenje_BERT/bert_multilingual_KONCEN/'\n",
        "\n",
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = BertForMaskedLM.from_pretrained(output_dir)\n",
        "#tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEIo3mUNtMeL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "dbe53c3d-6c82-446b-b330-7936e2da8dd9"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers.modeling_bert import BertForMaskedLM, BertModel\n",
        "\n",
        "import sys, re\n",
        "\n",
        "import time\n",
        "\n",
        "import os\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "!pip install torch\n",
        "import torch\n",
        "\n",
        "f = open(\"drive/My Drive/ucenje_BERT/testna_mnozica_bert/testna_mnozica_halozan/unzipped_testiraj_bert/vejica_stavki.txt\", \"r\")\n",
        "stavki = f.read()\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('drive/My Drive/ucenje_BERT/testna_mnozica_bert/testna_mnozica_halozan/unzipped_testiraj_bert/crosloengual-bert-pytorch/', do_lower_case=True)\n",
        "\n",
        "output_dir = 'drive/My Drive/ucenje_BERT/testna_mnozica_bert/testna_mnozica_halozan/unzipped_testiraj_bert/bert_model_naucen/'\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained(output_dir)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "\n",
        "predictedStavki = \"\"\n",
        "postavljeniStavki = \"\"\n",
        "dontOverkillStevec = 0\n",
        "\n",
        "stavek_za_med_postavljene = \"\"\n",
        "\n",
        "dolzina = len(stavki.split(\"\\n\")) - 1\n",
        "\n",
        "for stavek in stavki.split(\"\\n\"):\n",
        "    if dontOverkillStevec > dolzina:\n",
        "        break\n",
        "    dontOverkillStevec += 1\n",
        "\n",
        "    #len(stavki.split(\"\\n\"))\n",
        "    if dontOverkillStevec % 20 == 0:\n",
        "        print(\"progress:\", dontOverkillStevec / dolzina * 100)\n",
        "\n",
        "    if (len(stavek) > 1):\n",
        "        stavek_za_med_postavljene = stavek\n",
        "\n",
        "    stavek = re.sub(',', '', stavek)\n",
        "\n",
        "    originalen_stavek = stavek.split(\" \")\n",
        "\n",
        "    stavek = stavek.lower()\n",
        "    stavek = re.sub('[!@#$:;\\'\\\"“”•«»%&ø×¹]', '', stavek)\n",
        "    stavek = re.sub('[öó]', 'o', stavek)\n",
        "    stavek = re.sub('[áä]', 'a', stavek)\n",
        "    stavek = re.sub('[0123456789]', '1', stavek)\n",
        "    stavek = re.sub('ë', 'e', stavek)\n",
        "    stavek = re.sub('í', 'i', stavek)\n",
        "    stavek = re.sub('ü', 'u', stavek)\n",
        "    stavek = re.sub('ć', 'c', stavek)\n",
        "    stavek = re.sub('ž', 'z', stavek)\n",
        "    stavek = re.sub('č', 'c', stavek)\n",
        "    stavek = re.sub('š', 's', stavek)\n",
        "    stavek = re.sub('é', 'e', stavek)\n",
        "    stavek = re.sub('đ', 'd', stavek)\n",
        "\n",
        "    stavek = \"[CLS] \" + stavek + \" [SEP]\"\n",
        "\n",
        "    #print(stavek)\n",
        "\n",
        "    maskedStavek = \"\"\n",
        "    mimozacetka = False\n",
        "    for token in stavek.split(\" \"):\n",
        "        if token != \"[CLS]\" and token != \"[SEP]\" and token != \".\" and token != \"?\" and token != \"!\" and mimozacetka:\n",
        "            maskedStavek += \" [MASK] \"\n",
        "        elif token != \"[CLS]\" and token != \"[SEP]\":\n",
        "            mimozacetka = True\n",
        "        if token == \"[SEP]\" or token == \".\":\n",
        "            maskedStavek += \" \"\n",
        "        maskedStavek += token\n",
        "        if token == \"[CLS]\":\n",
        "            maskedStavek += \" \"\n",
        "\n",
        "\n",
        "    #print(maskedStavek)\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    label_list = []\n",
        "    # For every sentence...\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        maskedStavek,\n",
        "                        add_special_tokens = False,\n",
        "                        max_length = 128,\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                    )\n",
        "\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    predictions , true_labels = [], []\n",
        "\n",
        "    stevec = 0\n",
        "\n",
        "    num_right_predictions = 0\n",
        "    num_all_predictions = 0\n",
        "    num_right_comma_predictions = 0\n",
        "    num_all_commas = 0\n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_masks)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    input_ids = input_ids[0].numpy()\n",
        "\n",
        "    predicted_stavek = \"\"\n",
        "    index_maske = 0\n",
        "\n",
        "    origin_counter_stavek = 0\n",
        "    index_maske_cez_mejo = False\n",
        "\n",
        "    if len(maskedStavek.split(\" \")) > 127:\n",
        "        continue\n",
        "\n",
        "    for token in maskedStavek.split(\" \"):\n",
        "        #print(torch.argmax(logits[0, index_maske]).item())\n",
        "        if index_maske > 126:\n",
        "            index_maske_cez_mejo = True\n",
        "            break\n",
        "        if token != \"[CLS]\" and token != \"[SEP]\":\n",
        "            if token == \"[MASK]\":\n",
        "                while input_ids[index_maske] != 105:\n",
        "                    index_maske = index_maske + 1\n",
        "                    if index_maske > 126:\n",
        "                        index_maske_cez_mejo = True\n",
        "                        break\n",
        "                if torch.argmax(logits[0, index_maske]).item() == 1:\n",
        "                    predicted_stavek += \",\"\n",
        "                index_maske += 1\n",
        "            else:\n",
        "                if index_maske > 0 and token != \".\":\n",
        "                    predicted_stavek += \" \"\n",
        "                predicted_stavek += originalen_stavek[origin_counter_stavek]\n",
        "                origin_counter_stavek += 1\n",
        "\n",
        "    if index_maske_cez_mejo:\n",
        "        continue\n",
        "\n",
        "    if len(predicted_stavek) > 1 and len(stavek_za_med_postavljene) > 1:\n",
        "        predictedStavki += predicted_stavek + \"\\n\"\n",
        "        postavljeniStavki += stavek_za_med_postavljene + \"\\n\"\n",
        "    stavek_za_med_postavljene = \"\"\n",
        "\n",
        "    assert len(postavljeniStavki.split('\\n')) == len(predictedStavki.split('\\n'))\n",
        "\n",
        "#print(postavljeniStavki)\n",
        "#print(\"\\n\")\n",
        "#print(predictedStavki)\n",
        "\n",
        "if predictedStavki == \"\":\n",
        "    predictedStavki = \"Prosimo, vnesite daljše besedilo.\"\n",
        "\n",
        "i = 0\n",
        "splitPostavljeniStavki = postavljeniStavki.split('\\n')\n",
        "splitPredictedStavki = predictedStavki.split('\\n')\n",
        "\n",
        "stPravilnoPostavljenih = 0\n",
        "stOdvecnoPostavljenih = 0\n",
        "stPozabljenih = 0\n",
        "stVseh = 0\n",
        "\n",
        "\n",
        "for i in range(len(splitPostavljeniStavki)):\n",
        "    #print(splitPostavljeniStavki[i])\n",
        "    #print(splitPredictedStavki[i])\n",
        "    originalniStavekVejice = [n for n,x in enumerate(splitPostavljeniStavki[i]) if x==',']\n",
        "    pridictedStavekVejice = [n for n,x in enumerate(splitPredictedStavki[i]) if x==',']\n",
        "\n",
        "    indexVejiceOriginalni = 0\n",
        "    indexVejicePredicted = 0\n",
        "    pristevalnik = 0\n",
        "\n",
        "    stVseh += len(originalniStavekVejice)\n",
        "    while True:\n",
        "        if indexVejiceOriginalni >= len(originalniStavekVejice):\n",
        "            if indexVejicePredicted < len(pridictedStavekVejice):\n",
        "                stOdvecnoPostavljenih += len(pridictedStavekVejice) - indexVejicePredicted\n",
        "            break\n",
        "        if indexVejicePredicted >= len(pridictedStavekVejice):\n",
        "            stPozabljenih += len(originalniStavekVejice) - indexVejiceOriginalni\n",
        "            break\n",
        "        if originalniStavekVejice[indexVejiceOriginalni] + pristevalnik <= pridictedStavekVejice[indexVejicePredicted]:\n",
        "            if originalniStavekVejice[indexVejiceOriginalni] + pristevalnik in pridictedStavekVejice:\n",
        "                stPravilnoPostavljenih += 1\n",
        "                indexVejiceOriginalni += 1\n",
        "                indexVejicePredicted += 1\n",
        "            else:\n",
        "                pristevalnik -= 1\n",
        "                indexVejiceOriginalni += 1\n",
        "                stPozabljenih += 1\n",
        "        else:\n",
        "            stOdvecnoPostavljenih += 1\n",
        "            indexVejicePredicted += 1\n",
        "            pristevalnik += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"st vseh je:\", stVseh)\n",
        "print(\"st pravilno postavljenih je:\", stPravilnoPostavljenih)\n",
        "print(\"st odvecno postavljenih je:\", stOdvecnoPostavljenih)\n",
        "print(\"st pozabljenih je:\", stPozabljenih)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6ce949ae3214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mnum_all_commas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         )\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m         )\n\u001b[1;32m    839\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    490\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m                 )\n\u001b[1;32m    494\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m         )\n\u001b[1;32m    435\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1665\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}