{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99WuR8Cbv0Ue",
        "outputId": "111d7c02-c95a-40d4-9d68-d52e618f774d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (0.29.36)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.13)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement keras.layers.recurrent (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for keras.layers.recurrent\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting SentencePiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SentencePiece\n",
            "Successfully installed SentencePiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install Cython\n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install keras.layers.recurrent\n",
        "!pip install transformers\n",
        "!pip install SentencePiece\n",
        "\n",
        "import random\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from random import randint, choice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install happytransformer\n",
        "!pip install --upgrade accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9rBIxsUv-80",
        "outputId": "678e3f2c-b878-4150-d81e-a276098e26c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting happytransformer\n",
            "  Downloading happytransformer-2.4.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.43 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (4.65.0)\n",
            "Requirement already satisfied: transformers>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from happytransformer) (4.31.0)\n",
            "Collecting datasets>=1.6.0 (from happytransformer)\n",
            "  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from happytransformer) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from happytransformer) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=1.6.0->happytransformer)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (2.27.1)\n",
            "Collecting xxhash (from datasets>=1.6.0->happytransformer)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=1.6.0->happytransformer)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.6.0->happytransformer) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->happytransformer) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0->happytransformer) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0->happytransformer) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.4.0->happytransformer) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.4.0->happytransformer) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.4.0->happytransformer) (0.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.6.0->happytransformer) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0->happytransformer) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.6.0->happytransformer) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.6.0->happytransformer) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0->happytransformer) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.6.0->happytransformer) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets, happytransformer\n",
            "Successfully installed datasets-2.14.3 dill-0.3.7 happytransformer-2.4.1 multiprocess-0.70.15 xxhash-3.3.0\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri-ds1xGv2dX",
        "outputId": "b1315293-c79a-4569-aacc-58e514b4ceba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#os.chdir(\"/content/drive/My Drive/magistrska/\" + cur_mapa)\n",
        "data_folder = \"/content/drive/My Drive/magistrska/prepared_data/separated_lists/test_data.csv\""
      ],
      "metadata": {
        "id": "Pa2hOrVHv5ls"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file\n",
        "with open(data_folder, 'r') as input_file:\n",
        "    reader = csv.reader(input_file)\n",
        "    rows = list(reader)\n",
        "\n",
        "# Count the number of rows\n",
        "num_rows = len(rows)\n",
        "\n",
        "print(f'Number of rows: {num_rows}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhXdmt27v7Cl",
        "outputId": "05b02538-035e-4c65-c769-e9661ed3408d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 47491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_folder, 'r') as input_file:\n",
        "    reader = csv.reader(input_file)\n",
        "    rows = list(reader)\n",
        "\n",
        "\n",
        "# Write the second rows to a CSV file\n",
        "with open('eval.csv', 'w') as second_file:\n",
        "    writer = csv.writer(second_file)\n",
        "    # Write the header row\n",
        "    writer.writerow([\"input\", \"target\"])\n",
        "    writer.writerows(rows)"
      ],
      "metadata": {
        "id": "TKVkCOOyv7NQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from happytransformer import HappyTextToText\n",
        "\n",
        "# pretrained google mt5 model\n",
        "#happy_tt = HappyTextToText(\"T5\", \"google/mt5-base\")\n",
        "\n",
        "#pretrained sl t5 small model\n",
        "#happy_tt = HappyTextToText(\"T5\", \"t5-sl-small\", load_path=\"/content/drive/My Drive/magistrska/pretrained_models/t5-sl-small/t5-sl-small\")\n",
        "\n",
        "#pretrained sl t5 large model\n",
        "#happy_tt = HappyTextToText(\"T5\", \"cjvt/t5-sl-large\", load_path=\"/content/drive/My Drive/magistrska/pretrained_models/t5-sl-large\")\n",
        "\n",
        "\n",
        "\n",
        "#trained google mt5 base model\n",
        "#happy_tt = HappyTextToText(\"T5\", \"google/mt5-base\", load_path=\"/content/drive/My Drive/magistrska/ucenje_feb_15_model/\")\n",
        "\n",
        "#trained sl t5 small model\n",
        "happy_tt = HappyTextToText(\"T5\", \"t5-sl-small\", load_path=\"/content/drive/My Drive/magistrska/trained_models/char_t5_aug_3/\")\n",
        "\n",
        "#trained sl t5 large model\n",
        "#happy_tt = HappyTextToText(\"T5\", \"cjvt/t5-sl-large\", load_path=\"/content/drive/My Drive/magistrska/ucenje_mar_11_slo_t5_model_large_1\")\n",
        "\n",
        "#trained sl t5 on masked words\n",
        "#happy_tt = HappyTextToText(\"T5\", \"t5-sl-small\", load_path=\"/content/drive/My Drive/magistrska/trained_models/ucenje_maj_4_masked_words_1/\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q87duwHv7Pi",
        "outputId": "d0d4a398-5fa7-4951-b36b-ca5619fc4862"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from happytransformer import TTSettings\n",
        "\n",
        "beam_settings =  TTSettings(num_beams=5, min_length=1, max_length=400, top_p=10)"
      ],
      "metadata": {
        "id": "3w8Gw46tw2Q9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_words(word1, word2, matchPercentage=60):\n",
        "    word1_len = len(word1)\n",
        "    word2_len = len(word2)\n",
        "    min_len = min(word1_len, word2_len)\n",
        "    matching_chars = 0\n",
        "\n",
        "    for i in range(min_len):\n",
        "        if word1[i].casefold() == word2[i].casefold():\n",
        "            matching_chars += 1\n",
        "\n",
        "    match_percentage = (matching_chars / min_len) * 100\n",
        "    if match_percentage >= matchPercentage:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "FEr-8zKHv7SK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "limit_test_cases = True\n",
        "\n",
        "all_sentences = 0\n",
        "matched_wo_sentences = 0\n",
        "non_matched_wo_sentences = 0\n",
        "\n",
        "all_words = 0\n",
        "correct_words = 0\n",
        "incorrect_words = 0\n",
        "\n",
        "all_words_in_right_wo_sentences = 0\n",
        "correct_words_in_right_wo_sentences = 0\n",
        "incorrect_words_in_right_wo_sentences = 0\n",
        "\n",
        "all_wrong_right_words = 0\n",
        "correct_wrong_right_words = 0\n",
        "incorrect_wrong_right_words = 0\n",
        "\n",
        "TP = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "TN = 0\n",
        "\n",
        "with open(\"eval.csv\", \"r\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    counter = 0\n",
        "\n",
        "    for line in reader:\n",
        "      #we skip the firest line\n",
        "      if counter != 0:\n",
        "        result_line = happy_tt.generate_text(line[0], args=beam_settings)\n",
        "\n",
        "        result_line_split = result_line.text.split()\n",
        "        correct_line_split = line[1].split()\n",
        "\n",
        "        #Izlocimo besedo, ki oznacuje nalogo, ki jo model mora opraviti\n",
        "        incorrect_line_split = line[0].split()\n",
        "\n",
        "        for index in range(0, len(result_line_split)):\n",
        "          if index < len(result_line_split) and index < len(correct_line_split) and index < len(incorrect_line_split):\n",
        "            if correct_line_split[index] != incorrect_line_split[index]:\n",
        "              all_wrong_right_words += 1\n",
        "\n",
        "              if not compare_words(result_line_split[index], correct_line_split[index], 100):\n",
        "                incorrect_wrong_right_words += 1\n",
        "              else:\n",
        "                correct_wrong_right_words += 1\n",
        "\n",
        "        for index in range(0, len(result_line_split)):\n",
        "            all_words += 1\n",
        "\n",
        "            # print(\"correct_line_split: \", correct_line_split)\n",
        "            # print(\"incorrect_line_split: \", incorrect_line_split)\n",
        "            # print(\"result_line_split: \", result_line_split)\n",
        "\n",
        "            if index < len(incorrect_line_split) and index < len(correct_line_split):\n",
        "              # print(\"index: \", index)\n",
        "              # print(\"correct_line_split[index]: \", correct_line_split[index])\n",
        "              # print(\"incorrect_line_split[index]: \", incorrect_line_split[index])\n",
        "              # print(\"result_line_split[index]: \", result_line_split[index])\n",
        "\n",
        "              if correct_line_split[index] != incorrect_line_split[index] and compare_words(result_line_split[index], correct_line_split[index], 100):\n",
        "                #print(\"TP\")\n",
        "                TP += 1\n",
        "                print(\"correct_line_split: \", correct_line_split)\n",
        "                print(\"incorrect_line_split: \", incorrect_line_split)\n",
        "                print(\"result_line_split: \", result_line_split)\n",
        "              elif correct_line_split[index] != incorrect_line_split[index] and not compare_words(result_line_split[index], correct_line_split[index], 100):\n",
        "                # print(\"FN\")\n",
        "                FN += 1\n",
        "                # print(\"correct_line_split: \", correct_line_split)\n",
        "                # print(\"incorrect_line_split: \", incorrect_line_split)\n",
        "                # print(\"result_line_split: \", result_line_split)\n",
        "              elif correct_line_split[index] == incorrect_line_split[index] and compare_words(result_line_split[index], correct_line_split[index], 100):\n",
        "                #print(\"TN\")\n",
        "                TN += 1\n",
        "              elif correct_line_split[index] == incorrect_line_split[index] and not compare_words(result_line_split[index], correct_line_split[index], 100):\n",
        "                #print(\"FP\")\n",
        "                FP += 1\n",
        "                # print(\"correct_line_split: \", correct_line_split)\n",
        "                # print(\"incorrect_line_split: \", incorrect_line_split)\n",
        "                # print(\"result_line_split: \", result_line_split)\n",
        "              else:\n",
        "                print('NAPAKA')\n",
        "              # if not compare_words(result_line_split[index], correct_line_split[index], 100):\n",
        "              #   #print(\"padel pri: \", result_line_split[index], \" in \", correct_line_split[index])\n",
        "              #   incorrect_words += 1\n",
        "              # else:\n",
        "              #   correct_words += 1\n",
        "\n",
        "\n",
        "        all_same = 1\n",
        "\n",
        "        if len(result_line_split) == len(correct_line_split):\n",
        "          for index in range(0, len(result_line_split)):\n",
        "            all_words_in_right_wo_sentences += 1\n",
        "\n",
        "            if not compare_words(result_line_split[index], correct_line_split[index], 100):\n",
        "              #print(\"padel pri: \", result_line_split[index], \" in \", correct_line_split[index])\n",
        "              incorrect_words_in_right_wo_sentences += 1\n",
        "            else:\n",
        "              correct_words_in_right_wo_sentences += 1\n",
        "\n",
        "            if not compare_words(result_line_split[index], correct_line_split[index]):\n",
        "              all_same -= 1\n",
        "              #print(\"padel pri: \", result_line_split[index], \" in \", correct_line_split[index])\n",
        "\n",
        "              if all_same == 0:\n",
        "                break\n",
        "        else:\n",
        "          all_same = 0\n",
        "\n",
        "        if all_same > 0:\n",
        "          matched_wo_sentences += 1\n",
        "        else:\n",
        "          non_matched_wo_sentences += 1\n",
        "\n",
        "          #print(\"equal length, but different words\")\n",
        "          #print(result_line_split)\n",
        "          #print(correct_line_split)\n",
        "\n",
        "        all_sentences += 1\n",
        "\n",
        "      counter += 1\n",
        "      if limit_test_cases and counter > 100:\n",
        "        break\n",
        "\n",
        "print(\"all_sentences: \", all_sentences)\n",
        "print(\"matched_wo_sentences: \", matched_wo_sentences)\n",
        "print(\"non_matched_wo_sentences: \", non_matched_wo_sentences)\n",
        "\n",
        "print(\"all_words_in_right_wo_sentences: \", all_words_in_right_wo_sentences)\n",
        "print(\"correct_words_in_right_wo_sentences: \", correct_words_in_right_wo_sentences)\n",
        "print(\"incorrect_words_in_right_wo_sentences: \", incorrect_words_in_right_wo_sentences)\n",
        "\n",
        "print(\"all_words: \", all_words)\n",
        "print(\"correct_words:\", correct_words)\n",
        "print(\"incorrect_words:\", incorrect_words)\n",
        "\n",
        "print(\"all_wrong_right_words: \", all_wrong_right_words)\n",
        "print(\"correct_wrong_right_words: \", correct_wrong_right_words)\n",
        "print(\"incorrect_wrong_right_words: \", incorrect_wrong_right_words)"
      ],
      "metadata": {
        "id": "LssPUB6Nv7VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TP:\", TP)\n",
        "print(\"FP:\", FP)\n",
        "print(\"TN:\", TN)\n",
        "print(\"FN:\", FN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "fuZrhZSNw_Yg",
        "outputId": "f83278f3-8743-440b-ddd8-8b255480063f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1653a01706d4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TP:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FP:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TN:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FN:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TP' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_1 = \"na otok odšli z avtomobilom smo .\"\n",
        "result_1 = happy_tt.generate_text(example_1, args=beam_settings)\n",
        "print(result_1.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJyFOa291dDr",
        "outputId": "21480669-1b1f-4a11-da64-7f09f2350e30"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Na otok smo odšli z avtomobilom .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_1 = \"D v n č i c n _ d r u g a _ t e k m a _ v _ l i g i _ N B A _ č a k a _ v _ s k b o t o _ # _ k o _ b o _ t e k m e c _ M i n n s o t a _ # _ \"\n",
        "result_1 = happy_tt.generate_text(example_1, args=beam_settings)\n",
        "print(result_1.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx1NQIkx5fPE",
        "outputId": "1ad15c68-e0fd-4d7b-a32d-59388facc624"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D a n č i c a _ d r u g a _ t e k m a _ v _ l i g i _ N B A _ č a k a _ v _ s k r b o t o _ # _ k o _ b o _ t e k m e c _ M i n n s o t a _ # _\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_1 = \"grem domov v mesto novo ne .\"\n",
        "result_1 = happy_tt.generate_text(example_1, args=beam_settings)\n",
        "print(result_1.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEYGc8Rr9YHm",
        "outputId": "8fce9e70-33bd-447d-b083-ebefe53e4c41"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ne grem domov v Novo mesto .\n"
          ]
        }
      ]
    }
  ]
}